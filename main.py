import os
from dotenv import load_dotenv
import speech_recognition as sr
import cohere
from google.cloud import aiplatform
from google.oauth2 import service_account

load_dotenv()

recognizer = sr.Recognizer()

# Pre-recorded audio (.wav) file
# with sr.AudioFile("conversation.wav") as source:
#     audio = recognizer.record(source)
#     text = recognizer.recognize_google(audio)  # Free Google API for testing
#     print("Transcription:", text)
    
#     print('-----')

# Accessing system mic for audio
with sr.Microphone() as source:
    print("Listening...")
    audio = recognizer.listen(source, timeout=20)
    text = recognizer.recognize_google(audio)
    print("Transcription:", text)

# Inculcated Cohere to summarise text generated by the Google API
COHERE_API_KEY = os.getenv("COHERE_API_KEY")
co = cohere.Client(COHERE_API_KEY)
response = co.generate(
    prompt=f"Summarize this: {text}",
    max_tokens=50,
    temperature=0.7
)
summary = response.generations[0].text.strip()
print("Summary:", summary)

credentials = service_account.Credentials.from_service_account_file(
    "/Users/siraryanmichael/syncscribe/syncscribe-454510-6b6e3db310f5.json"
)
project_id = "syncscribe-454510"
location = "us-central1"

aiplatform.init(
    project=project_id,
    location=location,
    credentials=credentials
)

from vertexai.generative_models import GenerativeModel

# Initialize Vertex AI
aiplatform.init(
    project="syncscribe-454510",
    location="us-central1",
    credentials=credentials
)

# Use Gemini model instead of text-bison
model = GenerativeModel("gemini-pro")
response = model.generate_content(
    f"Provide insights on this meeting summary: {summary}"
)
insight = response.text
print("Insight:", insight)